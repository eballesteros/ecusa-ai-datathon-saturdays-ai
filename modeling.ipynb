{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ = '2.6.1'\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f\"{tf.__version__ = }\")\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "data_file_path = Path(\"data/COVID-19_Case_Surveillance_Public_Use_Data.csv\")\n",
    "\n",
    "# feature types\n",
    "feature_type = Enum(\"feature_type\", \"categorical continuous date\")\n",
    "\n",
    "# target name\n",
    "target_name = \"death_yn\"\n",
    "\n",
    "# these are my initial features, I will update this map\n",
    "# as I add or remove features\n",
    "feature_name=str\n",
    "feature_to_type_map: Dict[feature_name, feature_type] = {\n",
    "    \"cdc_report_dt\"                 : feature_type.date,\n",
    "    \"pos_spec_dt\"                   : feature_type.date,\n",
    "    \"onset_dt\"                      : feature_type.date,\n",
    "    \"current_status\"                : feature_type.categorical,\n",
    "    \"sex\"                           : feature_type.categorical,\n",
    "    \"age_group\"                     : feature_type.categorical,\n",
    "    \"Race and ethnicity (combined)\" : feature_type.categorical,\n",
    "    \"hosp_yn\"                       : feature_type.categorical,\n",
    "    \"icu_yn\"                        : feature_type.categorical,\n",
    "    \"death_yn\"                      : feature_type.categorical,\n",
    "    \"medcond_yn\"                    : feature_type.categorical,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29789/4005432254.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (8405079, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file_path)\n",
    "print(f\"{df.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race or ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from the original workshop notebook\n",
    "aux = df[\"Race and ethnicity (combined)\"].str.split(\",\", n = 1, expand = True)\n",
    "df[\"race\"] = aux[0]\n",
    "df[\"ethnicity\"] = aux[1]\n",
    "\n",
    "# update feature map\n",
    "feature_to_type_map[\"race\"] = feature_type.categorical\n",
    "feature_to_type_map[\"ethnicity\"] = feature_type.categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(s: pd.Series, **kwargs) -> pd.Series:\n",
    "    \"\"\"Thin wrapper around pd.to_datetime to only return date portion\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(s, **kwargs).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_report_dt = to_date(df[\"cdc_report_dt\"])\n",
    "pos_spec_dt = to_date(df[\"pos_spec_dt\"])\n",
    "onset_dt = to_date(df[\"onset_dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdc_report_dt.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fact that a date is missing can be a (binary) feature in inself\n",
    "pos_spec_dt_is_missing = pos_spec_dt.isna()\n",
    "onset_dt_is_missing = onset_dt.isna()\n",
    "\n",
    "# str to make it consistent with every other categorical in this ds\n",
    "df[\"pos_spec_dt_is_missing\"]=np.where(pos_spec_dt_is_missing, \"True\", \"False\")\n",
    "df[\"onset_dt_is_missing\"]=np.where(onset_dt_is_missing, \"True\", \"False\")\n",
    "\n",
    "feature_to_type_map[\"pos_spec_dt_is_missing\"]=feature_type.categorical\n",
    "feature_to_type_map[\"onset_dt_is_missing\"]=feature_type.categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29789/1113682547.py:7: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  pos_spec_dt[pos_spec_dt_is_missing] = cdc_report_dt[pos_spec_dt_is_missing] + pos_spec_dt_median_diff\n",
      "/tmp/ipykernel_29789/1113682547.py:8: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  onset_dt[onset_dt_is_missing] = cdc_report_dt[onset_dt_is_missing] + onset_dt_median_diff\n"
     ]
    }
   ],
   "source": [
    "# compute median difference with cdc_report when date is not missing\n",
    "# if we are being strict, I should compute the median with only samples from the triaing set\n",
    "pos_spec_dt_median_diff = (pos_spec_dt[~pos_spec_dt_is_missing] - cdc_report_dt[~pos_spec_dt_is_missing]).median()\n",
    "onset_dt_median_diff = (onset_dt[~onset_dt_is_missing] - cdc_report_dt[~onset_dt_is_missing]).median()\n",
    "\n",
    "# impute cdc_report date + median difference\n",
    "pos_spec_dt[pos_spec_dt_is_missing] = cdc_report_dt[pos_spec_dt_is_missing] + pos_spec_dt_median_diff\n",
    "onset_dt[onset_dt_is_missing] = cdc_report_dt[onset_dt_is_missing] + onset_dt_median_diff\n",
    "\n",
    "# sanity check: assert no missing values left\n",
    "assert not pos_spec_dt.isna().any()\n",
    "assert not onset_dt.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all 3 back in df (as pd dates)\n",
    "df[\"cdc_report_dt\"] = cdc_report_dt\n",
    "df[\"pos_spec_dt\"] = pos_spec_dt\n",
    "df[\"onset_dt\"] = onset_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_column(df: pd.DataFrame, column_name: str, feature_to_type_map: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Process date column in-place\n",
    "    Modifies feature_to_type_map inplace as well to reflect new features\n",
    "    \"\"\"\n",
    "    # pop column and transform it to datetime \n",
    "    date_column = pd.to_datetime(df.pop(column_name), errors='raise')\n",
    "    _ = feature_to_type_map.pop(column_name)\n",
    "\n",
    "    # decompose date\n",
    "    date_column_year       = date_column.dt.year\n",
    "    date_column_month      = date_column.dt.month\n",
    "    date_column_week       = date_column.dt.isocalendar().week\n",
    "    date_column_dayofmonth = date_column.dt.day\n",
    "    date_column_dayofyear  = date_column.dt.dayofyear\n",
    "    date_column_dayofweek  = date_column.dt.dayofweek #Monday=0, Sunday=6\n",
    "    date_column_elapsed    = (date_column - date_column.min()).dt.days\n",
    "\n",
    "    # encode cyclical features with sin/cos encoding\n",
    "    def encode_cyclical(values: pd.Series, feature_name: str) -> None:\n",
    "        \"\"\"Encode cyclical \n",
    "        \"\"\"\n",
    "        df[f\"{column_name}_{feature_name}_sin\"] = np.sin(2 * np.pi * values / values.max())\n",
    "        df[f\"{column_name}_{feature_name}_cos\"] = np.cos(2 * np.pi * values / values.max())\n",
    "\n",
    "        feature_to_type_map[f\"{column_name}_{feature_name}_sin\"]=feature_type.continuous\n",
    "        feature_to_type_map[f\"{column_name}_{feature_name}_cos\"]=feature_type.continuous\n",
    "\n",
    "    encode_cyclical(date_column_month, feature_name=\"month\")\n",
    "    encode_cyclical(date_column_week, feature_name=\"week\")\n",
    "    encode_cyclical(date_column_dayofmonth, feature_name=\"dayofmonth\")\n",
    "    encode_cyclical(date_column_dayofyear, feature_name=\"dayofyear\")\n",
    "    encode_cyclical(date_column_dayofweek, feature_name=\"dayofweek\")\n",
    "\n",
    "    # in addition, add month and year as categorical\n",
    "    df[f\"{column_name}_year\"] = date_column_year\n",
    "    df[f\"{column_name}_month\"] = date_column_month\n",
    "\n",
    "    feature_to_type_map[f\"{column_name}_year\"]=feature_type.continuous\n",
    "    feature_to_type_map[f\"{column_name}_month\"]=feature_type.continuous\n",
    "\n",
    "    # and elapsed as continuous\n",
    "    df[f\"{column_name}_elapsed\"] = date_column_elapsed\n",
    "    feature_to_type_map[f\"{column_name}_elapsed\"]=feature_type.continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = [f for f,t in feature_to_type_map.items() if t == feature_type.date]\n",
    "for cname in date_features:\n",
    "    process_date_column(df, column_name=cname, feature_to_type_map=feature_to_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [f for f,t in feature_to_type_map.items() if t == feature_type.continuous]\n",
    "categorical_features = [f for f,t in feature_to_type_map.items() if t == feature_type.categorical]\n",
    "\n",
    "# sanity check\n",
    "assert len(df.columns) == len(continuous_features) + len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in continuous_features:\n",
    "    assert not df[c].isna().any(), f\"Na found in {c}\"\n",
    "    df[c]=df[c].astype(np.float32)\n",
    "\n",
    "for c in categorical_features:\n",
    "    # NA will be just one more category\n",
    "    df[c]=df[c].fillna(\"#NA#\").astype(\"category\")\n",
    "\n",
    "    #sanity check\n",
    "    assert not df[c].isna().any(), f\"Na found in {c}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=.33, random_state=42) # important note about this at the end\n",
    "\n",
    "print(f\"{len(train_df) = }\")\n",
    "print(f\"{len(valid_df) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe: pd.DataFrame, target_name: str, shuffle: bool=True, batch_size: int=32):\n",
    "  df = dataframe.copy()\n",
    "  labels = df.pop(target_name)\n",
    "  df_dict = {key: value.to_numpy()[:,None] for key, value in dataframe.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((df_dict, labels))\n",
    "  if shuffle:\n",
    "    # set max buffer size of 100k to avoid blowing up the memory\n",
    "    # this may result in not perfect shuffles\n",
    "    ds = ds.shuffle(buffer_size=len(min(len(dataframe), 100_00)))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train_df, target_name=target_name, shuffle=True)\n",
    "valid_ds = df_to_dataset(valid_df, target_name=target_name, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model preprocessing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers\n",
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "def get_lookup_layer(name, dataset, max_tokens=None):\n",
    "  # create StringLookup layer for the feature\n",
    "  index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  \n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  return index\n",
    "\n",
    "def get_embedding_layer(lookup_layer):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  def emb_sz_rule(n_cat:int)->int: \n",
    "    \"\"\"\n",
    "    fast-ais rule of thumb for embedding size\n",
    "    https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608/2\n",
    "    \"\"\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "  pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "for f in continuous_features:\n",
    "    # define input\n",
    "    feature_input = keras.Input(shape=(1,), name=f)\n",
    "    all_inputs.append(feature_input)\n",
    "    \n",
    "    # add input prep layers\n",
    "    normalization_layer = get_normalization_layer(f, train_ds)\n",
    "    encoded_feature = normalization_layer(feature_input)\n",
    "    encoded_features.append(encoded_feature)\n",
    "\n",
    "for f in categorical_features:\n",
    "    # define input\n",
    "    feature_input = keras.Input(shape=(1,), name=f)\n",
    "    all_inputs.append(feature_input)\n",
    "\n",
    "    # add input prep layers\n",
    "    lookup_layer = get_lookup_layer(f, train_ds)\n",
    "    encoded_feature = lookup_layer(feature_input)\n",
    "\n",
    "\n",
    "    encoded_features.append(encoded_feature)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
