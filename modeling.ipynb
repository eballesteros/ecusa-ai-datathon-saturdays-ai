{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ = '2.6.1'\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"{tf.__version__ = }\")\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "data_file_path = Path(\"data/COVID-19_Case_Surveillance_Public_Use_Data.csv\")\n",
    "\n",
    "# feature types\n",
    "feature_type = Enum(\"feature_type\", \"categorical continuous date\")\n",
    "\n",
    "# target name\n",
    "target_name = \"death_yn\"\n",
    "\n",
    "# these are my initial features, I will update this map\n",
    "# as I add or remove features\n",
    "feature_name=str\n",
    "feature_to_type_map: Dict[feature_name, feature_type] = {\n",
    "    \"cdc_report_dt\"                 : feature_type.date,\n",
    "    \"pos_spec_dt\"                   : feature_type.date,\n",
    "    \"onset_dt\"                      : feature_type.date,\n",
    "    \"current_status\"                : feature_type.categorical,\n",
    "    \"sex\"                           : feature_type.categorical,\n",
    "    \"age_group\"                     : feature_type.categorical,\n",
    "    \"Race and ethnicity (combined)\" : feature_type.categorical,\n",
    "    \"hosp_yn\"                       : feature_type.categorical,\n",
    "    \"icu_yn\"                        : feature_type.categorical,\n",
    "    \"death_yn\"                      : feature_type.categorical,\n",
    "    \"medcond_yn\"                    : feature_type.categorical,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36437/4005432254.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (8405079, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file_path)\n",
    "print(f\"{df.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race or ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from the original workshop notebook\n",
    "aux = df[\"Race and ethnicity (combined)\"].str.split(\",\", n = 1, expand = True)\n",
    "df[\"race\"] = aux[0]\n",
    "df[\"ethnicity\"] = aux[1]\n",
    "\n",
    "# update feature map\n",
    "feature_to_type_map[\"race\"] = feature_type.categorical\n",
    "feature_to_type_map[\"ethnicity\"] = feature_type.categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(s: pd.Series, **kwargs) -> pd.Series:\n",
    "    \"\"\"Thin wrapper around pd.to_datetime to only return date portion\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(s, **kwargs).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_report_dt = to_date(df[\"cdc_report_dt\"])\n",
    "pos_spec_dt = to_date(df[\"pos_spec_dt\"])\n",
    "onset_dt = to_date(df[\"onset_dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdc_report_dt.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fact that a date is missing can be a (binary) feature in inself\n",
    "pos_spec_dt_is_missing = pos_spec_dt.isna()\n",
    "onset_dt_is_missing = onset_dt.isna()\n",
    "\n",
    "# str to make it consistent with every other categorical in this ds\n",
    "df[\"pos_spec_dt_is_missing\"]=np.where(pos_spec_dt_is_missing, \"True\", \"False\")\n",
    "df[\"onset_dt_is_missing\"]=np.where(onset_dt_is_missing, \"True\", \"False\")\n",
    "\n",
    "feature_to_type_map[\"pos_spec_dt_is_missing\"]=feature_type.categorical\n",
    "feature_to_type_map[\"onset_dt_is_missing\"]=feature_type.categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36437/1113682547.py:7: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  pos_spec_dt[pos_spec_dt_is_missing] = cdc_report_dt[pos_spec_dt_is_missing] + pos_spec_dt_median_diff\n",
      "/tmp/ipykernel_36437/1113682547.py:8: SettingWithCopyWarning: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.\n",
      "  onset_dt[onset_dt_is_missing] = cdc_report_dt[onset_dt_is_missing] + onset_dt_median_diff\n"
     ]
    }
   ],
   "source": [
    "# compute median difference with cdc_report when date is not missing\n",
    "# if we are being strict, I should compute the median with only samples from the triaing set\n",
    "pos_spec_dt_median_diff = (pos_spec_dt[~pos_spec_dt_is_missing] - cdc_report_dt[~pos_spec_dt_is_missing]).median()\n",
    "onset_dt_median_diff = (onset_dt[~onset_dt_is_missing] - cdc_report_dt[~onset_dt_is_missing]).median()\n",
    "\n",
    "# impute cdc_report date + median difference\n",
    "pos_spec_dt[pos_spec_dt_is_missing] = cdc_report_dt[pos_spec_dt_is_missing] + pos_spec_dt_median_diff\n",
    "onset_dt[onset_dt_is_missing] = cdc_report_dt[onset_dt_is_missing] + onset_dt_median_diff\n",
    "\n",
    "# sanity check: assert no missing values left\n",
    "assert not pos_spec_dt.isna().any()\n",
    "assert not onset_dt.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all 3 back in df (as pd dates)\n",
    "df[\"cdc_report_dt\"] = cdc_report_dt\n",
    "df[\"pos_spec_dt\"] = pos_spec_dt\n",
    "df[\"onset_dt\"] = onset_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_column(df: pd.DataFrame, column_name: str, feature_to_type_map: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Process date column in-place\n",
    "    Modifies feature_to_type_map inplace as well to reflect new features\n",
    "    \"\"\"\n",
    "    # pop column and transform it to datetime \n",
    "    date_column = pd.to_datetime(df.pop(column_name), errors='raise')\n",
    "    _ = feature_to_type_map.pop(column_name)\n",
    "\n",
    "    # decompose date\n",
    "    date_column_year       = date_column.dt.year\n",
    "    date_column_month      = date_column.dt.month\n",
    "    date_column_week       = date_column.dt.isocalendar().week\n",
    "    date_column_dayofmonth = date_column.dt.day\n",
    "    date_column_dayofyear  = date_column.dt.dayofyear\n",
    "    date_column_dayofweek  = date_column.dt.dayofweek #Monday=0, Sunday=6\n",
    "    date_column_elapsed    = (date_column - date_column.min()).dt.days\n",
    "\n",
    "    # encode cyclical features with sin/cos encoding\n",
    "    def encode_cyclical(values: pd.Series, feature_name: str) -> None:\n",
    "        \"\"\"Encode cyclical \n",
    "        \"\"\"\n",
    "        df[f\"{column_name}_{feature_name}_sin\"] = np.sin(2 * np.pi * values / values.max())\n",
    "        df[f\"{column_name}_{feature_name}_cos\"] = np.cos(2 * np.pi * values / values.max())\n",
    "\n",
    "        feature_to_type_map[f\"{column_name}_{feature_name}_sin\"]=feature_type.continuous\n",
    "        feature_to_type_map[f\"{column_name}_{feature_name}_cos\"]=feature_type.continuous\n",
    "\n",
    "    encode_cyclical(date_column_month, feature_name=\"month\")\n",
    "    encode_cyclical(date_column_week, feature_name=\"week\")\n",
    "    encode_cyclical(date_column_dayofmonth, feature_name=\"dayofmonth\")\n",
    "    encode_cyclical(date_column_dayofyear, feature_name=\"dayofyear\")\n",
    "    encode_cyclical(date_column_dayofweek, feature_name=\"dayofweek\")\n",
    "\n",
    "    # in addition, add month and year as categorical\n",
    "    df[f\"{column_name}_year\"] = date_column_year\n",
    "    df[f\"{column_name}_month\"] = date_column_month\n",
    "\n",
    "    feature_to_type_map[f\"{column_name}_year\"]=feature_type.continuous\n",
    "    feature_to_type_map[f\"{column_name}_month\"]=feature_type.continuous\n",
    "\n",
    "    # and elapsed as continuous\n",
    "    df[f\"{column_name}_elapsed\"] = date_column_elapsed\n",
    "    feature_to_type_map[f\"{column_name}_elapsed\"]=feature_type.continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = [f for f,t in feature_to_type_map.items() if t == feature_type.date]\n",
    "for cname in date_features:\n",
    "    process_date_column(df, column_name=cname, feature_to_type_map=feature_to_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [f for f,t in feature_to_type_map.items() if t == feature_type.continuous]\n",
    "categorical_features = [f for f,t in feature_to_type_map.items() if t == feature_type.categorical]\n",
    "\n",
    "# sanity check\n",
    "assert len(df.columns) == len(continuous_features) + len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in continuous_features:\n",
    "    assert not df[c].isna().any(), f\"Na found in {c}\"\n",
    "    df[c]=df[c].astype(np.float32)\n",
    "\n",
    "for c in categorical_features:\n",
    "    # NA will be just one more category\n",
    "    df[c]=df[c].fillna(\"#NA#\").astype(\"category\")\n",
    "\n",
    "    #sanity check\n",
    "    assert not df[c].isna().any(), f\"Na found in {c}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_status</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>Race and ethnicity (combined)</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>medcond_yn</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>pos_spec_dt_is_missing</th>\n",
       "      <th>onset_dt_is_missing</th>\n",
       "      <th>cdc_report_dt_month_sin</th>\n",
       "      <th>cdc_report_dt_month_cos</th>\n",
       "      <th>cdc_report_dt_week_sin</th>\n",
       "      <th>cdc_report_dt_week_cos</th>\n",
       "      <th>cdc_report_dt_dayofmonth_sin</th>\n",
       "      <th>cdc_report_dt_dayofmonth_cos</th>\n",
       "      <th>cdc_report_dt_dayofyear_sin</th>\n",
       "      <th>cdc_report_dt_dayofyear_cos</th>\n",
       "      <th>cdc_report_dt_dayofweek_sin</th>\n",
       "      <th>cdc_report_dt_dayofweek_cos</th>\n",
       "      <th>cdc_report_dt_year</th>\n",
       "      <th>cdc_report_dt_month</th>\n",
       "      <th>cdc_report_dt_elapsed</th>\n",
       "      <th>pos_spec_dt_month_sin</th>\n",
       "      <th>pos_spec_dt_month_cos</th>\n",
       "      <th>pos_spec_dt_week_sin</th>\n",
       "      <th>pos_spec_dt_week_cos</th>\n",
       "      <th>pos_spec_dt_dayofmonth_sin</th>\n",
       "      <th>pos_spec_dt_dayofmonth_cos</th>\n",
       "      <th>pos_spec_dt_dayofyear_sin</th>\n",
       "      <th>pos_spec_dt_dayofyear_cos</th>\n",
       "      <th>pos_spec_dt_dayofweek_sin</th>\n",
       "      <th>pos_spec_dt_dayofweek_cos</th>\n",
       "      <th>pos_spec_dt_year</th>\n",
       "      <th>pos_spec_dt_month</th>\n",
       "      <th>pos_spec_dt_elapsed</th>\n",
       "      <th>onset_dt_month_sin</th>\n",
       "      <th>onset_dt_month_cos</th>\n",
       "      <th>onset_dt_week_sin</th>\n",
       "      <th>onset_dt_week_cos</th>\n",
       "      <th>onset_dt_dayofmonth_sin</th>\n",
       "      <th>onset_dt_dayofmonth_cos</th>\n",
       "      <th>onset_dt_dayofyear_sin</th>\n",
       "      <th>onset_dt_dayofyear_cos</th>\n",
       "      <th>onset_dt_dayofweek_sin</th>\n",
       "      <th>onset_dt_dayofweek_cos</th>\n",
       "      <th>onset_dt_year</th>\n",
       "      <th>onset_dt_month</th>\n",
       "      <th>onset_dt_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Black, Non-Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.332870e-01</td>\n",
       "      <td>0.991077</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-1.736482e-01</td>\n",
       "      <td>0.984808</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.758306</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.748511</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>-0.789565</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Black, Non-Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.332870e-01</td>\n",
       "      <td>0.991077</td>\n",
       "      <td>0.299363</td>\n",
       "      <td>-0.954139</td>\n",
       "      <td>-9.681087e-02</td>\n",
       "      <td>0.995303</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.758306</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.663123</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.696196</td>\n",
       "      <td>0.717852</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Black, Non-Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.432490e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.651372</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.758306</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.663123</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>0.968077</td>\n",
       "      <td>-0.250653</td>\n",
       "      <td>-0.708652</td>\n",
       "      <td>0.705558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Black, Non-Hispanic</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.332870e-01</td>\n",
       "      <td>0.991077</td>\n",
       "      <td>0.299363</td>\n",
       "      <td>-0.954139</td>\n",
       "      <td>-9.681087e-02</td>\n",
       "      <td>0.995303</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.758306</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.748511</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>0.937752</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>-0.744704</td>\n",
       "      <td>0.667395</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Black, Non-Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.332870e-01</td>\n",
       "      <td>0.991077</td>\n",
       "      <td>0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-1.160929e-01</td>\n",
       "      <td>0.993238</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.758306</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.663123</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>-0.440394</td>\n",
       "      <td>-0.696196</td>\n",
       "      <td>0.717852</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              current_status   sex      age_group  \\\n",
       "0  Laboratory-confirmed case  Male  10 - 19 Years   \n",
       "1  Laboratory-confirmed case  Male  10 - 19 Years   \n",
       "2  Laboratory-confirmed case  Male  10 - 19 Years   \n",
       "3  Laboratory-confirmed case  Male  10 - 19 Years   \n",
       "4  Laboratory-confirmed case  Male  10 - 19 Years   \n",
       "\n",
       "  Race and ethnicity (combined)  hosp_yn   icu_yn death_yn medcond_yn   race  \\\n",
       "0           Black, Non-Hispanic       No  Unknown       No         No  Black   \n",
       "1           Black, Non-Hispanic       No       No       No         No  Black   \n",
       "2           Black, Non-Hispanic       No       No       No         No  Black   \n",
       "3           Black, Non-Hispanic  Missing  Missing       No    Missing  Black   \n",
       "4           Black, Non-Hispanic       No       No       No        Yes  Black   \n",
       "\n",
       "       ethnicity pos_spec_dt_is_missing onset_dt_is_missing  \\\n",
       "0   Non-Hispanic                  False                True   \n",
       "1   Non-Hispanic                  False               False   \n",
       "2   Non-Hispanic                  False               False   \n",
       "3   Non-Hispanic                  False                True   \n",
       "4   Non-Hispanic                  False               False   \n",
       "\n",
       "   cdc_report_dt_month_sin  cdc_report_dt_month_cos  cdc_report_dt_week_sin  \\\n",
       "0            -1.133108e-15                      1.0           -1.332870e-01   \n",
       "1            -1.133108e-15                      1.0           -1.332870e-01   \n",
       "2            -1.133108e-15                      1.0            6.432490e-16   \n",
       "3            -1.133108e-15                      1.0           -1.332870e-01   \n",
       "4            -1.133108e-15                      1.0           -1.332870e-01   \n",
       "\n",
       "   cdc_report_dt_week_cos  cdc_report_dt_dayofmonth_sin  \\\n",
       "0                0.991077                      0.897805   \n",
       "1                0.991077                      0.299363   \n",
       "2                1.000000                     -0.651372   \n",
       "3                0.991077                      0.299363   \n",
       "4                0.991077                      0.485302   \n",
       "\n",
       "   cdc_report_dt_dayofmonth_cos  cdc_report_dt_dayofyear_sin  \\\n",
       "0                     -0.440394                -1.736482e-01   \n",
       "1                     -0.954139                -9.681087e-02   \n",
       "2                     -0.758758                -2.449294e-16   \n",
       "3                     -0.954139                -9.681087e-02   \n",
       "4                     -0.874347                -1.160929e-01   \n",
       "\n",
       "   cdc_report_dt_dayofyear_cos  cdc_report_dt_dayofweek_sin  \\\n",
       "0                     0.984808                 8.660254e-01   \n",
       "1                     0.995303                -8.660254e-01   \n",
       "2                     1.000000                 1.224647e-16   \n",
       "3                     0.995303                -8.660254e-01   \n",
       "4                     0.993238                -8.660254e-01   \n",
       "\n",
       "   cdc_report_dt_dayofweek_cos  cdc_report_dt_year  cdc_report_dt_month  \\\n",
       "0                          0.5              2020.0                 11.0   \n",
       "1                          0.5              2020.0                 11.0   \n",
       "2                         -1.0              2020.0                 11.0   \n",
       "3                          0.5              2020.0                 11.0   \n",
       "4                         -0.5              2020.0                 11.0   \n",
       "\n",
       "   cdc_report_dt_elapsed  pos_spec_dt_month_sin  pos_spec_dt_month_cos  \\\n",
       "0                  314.0                   -0.5               0.866025   \n",
       "1                  318.0                   -0.5               0.866025   \n",
       "2                  323.0                   -0.5               0.866025   \n",
       "3                  318.0                   -0.5               0.866025   \n",
       "4                  317.0                   -0.5               0.866025   \n",
       "\n",
       "   pos_spec_dt_week_sin  pos_spec_dt_week_cos  pos_spec_dt_dayofmonth_sin  \\\n",
       "0             -0.375267              0.926917                    0.897805   \n",
       "1             -0.375267              0.926917                    0.897805   \n",
       "2             -0.375267              0.926917                    0.897805   \n",
       "3             -0.375267              0.926917                    0.897805   \n",
       "4             -0.375267              0.926917                    0.897805   \n",
       "\n",
       "   pos_spec_dt_dayofmonth_cos  pos_spec_dt_dayofyear_sin  \\\n",
       "0                   -0.440394                  -0.758306   \n",
       "1                   -0.440394                  -0.758306   \n",
       "2                   -0.440394                  -0.758306   \n",
       "3                   -0.440394                  -0.758306   \n",
       "4                   -0.440394                  -0.758306   \n",
       "\n",
       "   pos_spec_dt_dayofyear_cos  pos_spec_dt_dayofweek_sin  \\\n",
       "0                   0.651899                   0.866025   \n",
       "1                   0.651899                   0.866025   \n",
       "2                   0.651899                   0.866025   \n",
       "3                   0.651899                   0.866025   \n",
       "4                   0.651899                   0.866025   \n",
       "\n",
       "   pos_spec_dt_dayofweek_cos  pos_spec_dt_year  pos_spec_dt_month  \\\n",
       "0                        0.5            2020.0               11.0   \n",
       "1                        0.5            2020.0               11.0   \n",
       "2                        0.5            2020.0               11.0   \n",
       "3                        0.5            2020.0               11.0   \n",
       "4                        0.5            2020.0               11.0   \n",
       "\n",
       "   pos_spec_dt_elapsed  onset_dt_month_sin  onset_dt_month_cos  \\\n",
       "0                316.0                -0.5            0.866025   \n",
       "1                316.0                -0.5            0.866025   \n",
       "2                316.0                -0.5            0.866025   \n",
       "3                316.0                -0.5            0.866025   \n",
       "4                316.0                -0.5            0.866025   \n",
       "\n",
       "   onset_dt_week_sin  onset_dt_week_cos  onset_dt_dayofmonth_sin  \\\n",
       "0          -0.748511           0.663123                 0.394356   \n",
       "1          -0.663123           0.748511                 0.897805   \n",
       "2          -0.663123           0.748511                 0.968077   \n",
       "3          -0.748511           0.663123                 0.937752   \n",
       "4          -0.663123           0.748511                 0.897805   \n",
       "\n",
       "   onset_dt_dayofmonth_cos  onset_dt_dayofyear_sin  onset_dt_dayofyear_cos  \\\n",
       "0                 0.918958               -0.789565                0.613667   \n",
       "1                -0.440394               -0.696196                0.717852   \n",
       "2                -0.250653               -0.708652                0.705558   \n",
       "3                 0.347305               -0.744704                0.667395   \n",
       "4                -0.440394               -0.696196                0.717852   \n",
       "\n",
       "   onset_dt_dayofweek_sin  onset_dt_dayofweek_cos  onset_dt_year  \\\n",
       "0                0.000000                     1.0         2020.0   \n",
       "1                0.866025                     0.5         2020.0   \n",
       "2                0.000000                     1.0         2020.0   \n",
       "3               -0.866025                    -0.5         2020.0   \n",
       "4                0.866025                     0.5         2020.0   \n",
       "\n",
       "   onset_dt_month  onset_dt_elapsed  \n",
       "0            11.0             313.0  \n",
       "1            11.0             321.0  \n",
       "2            11.0             320.0  \n",
       "3            11.0             317.0  \n",
       "4            11.0             321.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df) = 5631402\n",
      "len(valid_df) = 2773677\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=.33, random_state=42) # important note about this at the end\n",
    "\n",
    "print(f\"{len(train_df) = }\")\n",
    "print(f\"{len(valid_df) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe: pd.DataFrame, target_name: str, shuffle: bool=True, batch_size: int=32):\n",
    "  df = dataframe.copy()\n",
    "  labels = df.pop(target_name)\n",
    "  df_dict = {key: value.to_numpy()[:,None] for key, value in dataframe.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((df_dict, labels))\n",
    "  if shuffle:\n",
    "    # set max buffer size of 100k to avoid blowing up the memory\n",
    "    # this may result in not perfect shuffles\n",
    "    ds = ds.shuffle(buffer_size=min(len(dataframe), 100_00))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 02:19:00.385576: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-03 02:19:00.385627: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 88654ae2549f\n",
      "2022-04-03 02:19:00.385639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 88654ae2549f\n",
      "2022-04-03 02:19:00.385782: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.82.1\n",
      "2022-04-03 02:19:00.385815: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.82.1\n",
      "2022-04-03 02:19:00.385825: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.82.1\n",
      "2022-04-03 02:19:00.386170: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = df_to_dataset(train_df, target_name=target_name, shuffle=True)\n",
    "valid_ds = df_to_dataset(valid_df, target_name=target_name, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model preprocessing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers\n",
    "def get_normalization_layer(name: str, dataframe: pd.DataFrame):\n",
    "  \"\"\"prep layer for continuous inputs\n",
    "  \"\"\"\n",
    "  # I prefer computing these from pandas instead of calling\n",
    "  # layer.adapt because it's way faster\n",
    "  mean = dataframe[name].mean()\n",
    "  var = dataframe[name].var()\n",
    "  \n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None, mean=mean, variance=var)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "def get_lookup_layer(name: str, dataframe: pd.DataFrame, max_tokens=None):\n",
    "  \"\"\"prep layer for categorical inputs (str->int)\n",
    "  \"\"\"\n",
    "  # I prefer computing these from pandas instead of calling\n",
    "  # layer.adapt because it's way faster\n",
    "  vocab = dataframe[name].unique()\n",
    "\n",
    "  # create StringLookup layer for the feature\n",
    "  index = layers.StringLookup(max_tokens=max_tokens)\n",
    "\n",
    "  return index\n",
    "\n",
    "def get_embedding_layer(lookup_layer, size_multiplier=1):\n",
    "  \"\"\"learned dimensionality reduction layer for categorical inputs\n",
    "  \"\"\"\n",
    "  def emb_sz_rule(n_cat:int)->int: \n",
    "    \"\"\"\n",
    "    fast-ais rule of thumb for embedding size\n",
    "    https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608/2\n",
    "    \"\"\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "  \n",
    "  vocab_size = lookup_layer.vocabulary_size()\n",
    "  output_size = size_multiplier * emb_sz_rule(vocab_size)\n",
    "  embedding_layer = layers.Embedding(vocab_size, output_size)\n",
    "  return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 24.88it/s]\n",
      "100%|██████████| 39/39 [00:02<00:00, 17.28it/s]\n"
     ]
    }
   ],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "for f in tqdm(categorical_features):\n",
    "    # define input\n",
    "    feature_input = keras.Input(shape=(1,), name=f)\n",
    "    all_inputs.append(feature_input)\n",
    "\n",
    "    # add lookup+embedding layers\n",
    "    lookup_layer = get_lookup_layer(f, train_df)\n",
    "    embedding_layer = get_embedding_layer(lookup_layer)\n",
    "\n",
    "    encoded_feature = embedding_layer(lookup_layer(feature_input))\n",
    "    encoded_features.append(encoded_feature)\n",
    "\n",
    "for f in tqdm(continuous_features):\n",
    "    # define input\n",
    "    feature_input = keras.Input(shape=(1,), name=f)\n",
    "    all_inputs.append(feature_input)\n",
    "    \n",
    "    # add normalization layer\n",
    "    normalization_layer = get_normalization_layer(f, train_df)\n",
    "    encoded_feature = normalization_layer(feature_input)\n",
    "    encoded_features.append(encoded_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
